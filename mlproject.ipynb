{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all my stuff\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        token_seq , tag_seq , current_token , current_tag = [],[],[],[]\n",
    "        for line in file:\n",
    "            token_tag = line.strip().split(\" \")\n",
    "\n",
    "            if len(token_tag) == 2:\n",
    "                current_token += [token_tag[0]]\n",
    "                current_tag += [token_tag[1]]\n",
    "            else:\n",
    "                token_seq += [current_token]\n",
    "                tag_seq += [current_tag]\n",
    "                current_token = []\n",
    "                current_tag = []\n",
    "        if (len(current_token) != 0):\n",
    "            token_seq += [current_token]\n",
    "            tag_seq += [current_tag]\n",
    "    return token_seq, tag_seq\n",
    "\n",
    "def read_test(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        token_seq , current_token = [],[]\n",
    "        for line in file:\n",
    "            token_test = line.strip()\n",
    "\n",
    "            if len(token_test) !=0:\n",
    "                current_token += [token_test]\n",
    "                \n",
    "            else:\n",
    "                token_seq += [current_token]\n",
    "                current_token = []  \n",
    "\n",
    "        if (len(current_token) != 0):\n",
    "            token_seq += [current_token]\n",
    "            \n",
    "    return token_seq\n",
    "\n",
    "def create_emission_dictionary(token_seq, tag_seq,k):\n",
    "    emission_dictionary = {} \n",
    "    count_tag = {} #number of words tagged with tag\n",
    "    count_token_tagged_tag = {} #number of times a token is tagged with tag\n",
    "    for i in range(0,len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not (y in count_tag.keys()):\n",
    "                count_tag[y] = 0\n",
    "\n",
    "            count_tag[y] += 1\n",
    "\n",
    "            if not (x in count_token_tagged_tag.keys()):\n",
    "                count_token_tagged_tag[x] = {}\n",
    "\n",
    "            if not (y in count_token_tagged_tag[x].keys()):\n",
    "                count_token_tagged_tag[x][y] = 0\n",
    "\n",
    "            count_token_tagged_tag[x][y] += 1\n",
    "    \n",
    "    for i in range(0, len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not(x in emission_dictionary.keys()):\n",
    "                emission_dictionary[x] = {}\n",
    "            emission_dictionary[x][y] = (count_token_tagged_tag[x][y]) / (count_tag[y] + k)\n",
    "            emission_dictionary[x][\"START\"] = 0\n",
    "            emission_dictionary[x][\"STOP\"] = 0\n",
    "\n",
    "    emission_dictionary[\"#UNK#\"] = {}\n",
    "    for tag in count_tag.keys():\n",
    "        emission_dictionary[\"#UNK#\"][tag] = k / (count_tag[tag] + k)\n",
    "    uniquetags = list(count_tag.keys())\n",
    "    return emission_dictionary , uniquetags\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if not (tag in uniquetags):\n",
    "        return 0\n",
    "    elif not (token in emission_dictionary.keys()):\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    else:\n",
    "        if not (tag in emission_dictionary[token].keys()):\n",
    "            emission_dictionary[token][tag] = 0\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "\n",
    "def simple_sentiment_analysis(test_token,emission_dictionary, uniquetags):\n",
    "    predicted_tags = []\n",
    "    for token_seq in test_token:\n",
    "        current_token = []\n",
    "        for token in token_seq:\n",
    "            predicted_tag = \"\"\n",
    "            max_prob = 0\n",
    "            for tag in uniquetags:\n",
    "                prob = emission(token, tag, emission_dictionary, uniquetags)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    predicted_tag = tag\n",
    "            current_token += [predicted_tag]\n",
    "        predicted_tags += [current_token]\n",
    "        current_token = []\n",
    "    return predicted_tags\n",
    "\n",
    "def writeoutput(filename, tokens, predicted_tags):\n",
    "  with open(filename, 'w', encoding='utf-8') as file:\n",
    "\n",
    "    for i in range(0,len(tokens)):\n",
    "      for j in range(0,len(tokens[i])):\n",
    "        file.write(tokens[i][j] + \" \" + predicted_tags[i][j] + \"\\n\" )\n",
    "\n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1466\n",
      "\n",
      "#Correct Entity : 178\n",
      "Entity  precision: 0.1214\n",
      "Entity  recall: 0.7773\n",
      "Entity  F: 0.2100\n",
      "\n",
      "#Correct Sentiment : 97\n",
      "Sentiment  precision: 0.0662\n",
      "Sentiment  recall: 0.4236\n",
      "Sentiment  F: 0.1145\n"
     ]
    }
   ],
   "source": [
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "es_emission_dictionary, es_unique_tags = create_emission_dictionary(es_token_seq, es_tag_seq, 1)\n",
    "es_test_token_seq = read_test(\"ES/dev.in\")\n",
    "\n",
    "es_predicted_tags = simple_sentiment_analysis(es_test_token_seq, es_emission_dictionary, es_unique_tags)\n",
    "\n",
    "writeoutput(\"ES/dev.p1.out\", es_test_token_seq, es_predicted_tags)\n",
    "\n",
    "!python \"evalResult.py\" \"ES/dev.out\" \"ES/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1816\n",
      "\n",
      "#Correct Entity : 266\n",
      "Entity  precision: 0.1465\n",
      "Entity  recall: 0.6838\n",
      "Entity  F: 0.2413\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.0710\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.1170\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")\n",
    "\n",
    "ru_predicted_tags = simple_sentiment_analysis(ru_test_token_seq, ru_emission_dictionary, ru_unique_tags)\n",
    "\n",
    "writeoutput(\"RU/dev.p1.out\", ru_test_token_seq, ru_predicted_tags)\n",
    "\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p1.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in my array of tags and return dictionary that gives trasnsition param\n",
    "\n",
    "def create_transition_dictionary(tag_seq, unique_tags):\n",
    "    transition_dictionary = {}\n",
    "\n",
    "    for first_tag in unique_tags:\n",
    "        for second_tag in unique_tags:\n",
    "            count, total = 0, 0\n",
    "            for i in tag_seq:\n",
    "                total += len(i) - 1\n",
    "                for j in range(len(i) - 1):\n",
    "                    if i[j] == first_tag and i[j+1] == second_tag:\n",
    "                        count += 1\n",
    "            if count != 0:\n",
    "                transition_dictionary[(first_tag, second_tag)] = count / total\n",
    "\n",
    "    start_dict = Counter(i[0] for i in tag_seq)\n",
    "    stop_dict = Counter(i[-1] for i in tag_seq)\n",
    "\n",
    "    total_sentences = len(tag_seq)\n",
    "\n",
    "    for tag, count in start_dict.items():\n",
    "        transition_dictionary[('START', tag)] = count / total_sentences\n",
    "\n",
    "    for tag, count in stop_dict.items():\n",
    "        transition_dictionary[(tag, 'STOP')] = count / total_sentences\n",
    "\n",
    "    return transition_dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(transition_dict, previous_tag, current_tag):\n",
    "    transition_key = (previous_tag, current_tag)\n",
    "    \n",
    "    if transition_key not in transition_dict:\n",
    "        transition_dict[transition_key] = math.log(1)\n",
    "    \n",
    "    return transition_dict[transition_key]\n",
    "\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if tag not in uniquetags:\n",
    "        return math.log(1)\n",
    "    elif token not in emission_dictionary:\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    elif tag not in emission_dictionary[token]:\n",
    "        return math.log(1)\n",
    "    else:\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "\n",
    "def viterbi(test_token_seq_sentence, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    viterbi_probs = {0: {\"START\": math.log(1)}, **{i: {tag: -math.inf for tag in unique_tags} for i in range(1, len(test_token_seq_sentence) + 1)}}\n",
    "    \n",
    "    # Calculate probabilities for each tag in the first word\n",
    "    for tag in unique_tags:\n",
    "        emission_prob = emission(test_token_seq_sentence[0], tag, emission_dictionary, unique_tags)\n",
    "        transition_prob = transition(transition_dictionary, \"START\", tag)\n",
    "        \n",
    "        if emission_prob == 0 or transition_prob == 0:\n",
    "            viterbi_probs[1][tag] = -math.inf\n",
    "        else:\n",
    "            viterbi_probs[1][tag] = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[0][\"START\"]\n",
    "            \n",
    "    # Calculate probabilities for each tag in the rest of the words\n",
    "    for i in range(1, len(test_token_seq_sentence)):\n",
    "        next_token = test_token_seq_sentence[i]\n",
    "        for j in unique_tags:\n",
    "            max_prob = -math.inf\n",
    "            for k in unique_tags:\n",
    "                emission_prob = emission(next_token, j, emission_dictionary, unique_tags)\n",
    "                transition_prob = transition(transition_dictionary, k, j)\n",
    "                \n",
    "                if emission_prob == 0 or transition_prob == 0:\n",
    "                    prob = -math.inf\n",
    "                else:\n",
    "                    prob = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[i][k]\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "            viterbi_probs[i+1][j] = max_prob\n",
    "    \n",
    "    # Calculate probabilities for transitioning to the \"STOP\" tag\n",
    "    max_prob = -math.inf\n",
    "    for tag in unique_tags:\n",
    "        previous_prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = transition(transition_dictionary, tag, \"STOP\")\n",
    "        \n",
    "        if transition_prob == 0:\n",
    "            prob = -math.inf\n",
    "        else:\n",
    "            prob = previous_prob + math.log(transition_prob)\n",
    "        \n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1] = {}\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1][\"STOP\"] = max_prob\n",
    "    \n",
    "    # Backtracking to find the best path\n",
    "    best_path = []\n",
    "    argmax = -math.inf\n",
    "    currentmax = -math.inf\n",
    "    argmax_tag = \"NULL\"\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = transition(transition_dictionary, tag, \"STOP\")\n",
    "        if transition_prob != 0:\n",
    "            currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "\n",
    "    best_path.append(argmax_tag)\n",
    "    \n",
    "    # Backpropagation\n",
    "    for i in range(len(test_token_seq_sentence), 1, -1):\n",
    "        argmax = -math.inf\n",
    "        currentmax = math.log(1)\n",
    "        \n",
    "        for tag in unique_tags:\n",
    "            prob = viterbi_probs[i-1][tag]\n",
    "            transition_prob = transition(transition_dictionary, tag, best_path[-1])\n",
    "            \n",
    "            if transition_prob == 0:\n",
    "                currentmax = -math.inf\n",
    "            else:\n",
    "                currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "        best_path.append(argmax_tag)\n",
    "\n",
    "    best_path.reverse()\n",
    "    return best_path\n",
    "\n",
    "def viterbi_loop(test_token_seq, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    result = []\n",
    "\n",
    "    for sentence in test_token_seq:\n",
    "        viterbi_tags = viterbi(sentence, unique_tags, emission_dictionary, transition_dictionary)\n",
    "        updated_tags = handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary)\n",
    "        result.append(updated_tags)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary):\n",
    "    updated_tags = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if viterbi_tags[i] == \"NULL\":\n",
    "            word = sentence[i]\n",
    "            max_emission_value = -math.inf\n",
    "            best_emission_tag = None\n",
    "\n",
    "            for tag in unique_tags:\n",
    "                emission_prob = emission(word, tag, emission_dictionary, unique_tags)\n",
    "                if emission_prob > max_emission_value:\n",
    "                    max_emission_value = emission_prob\n",
    "                    best_emission_tag = tag\n",
    "\n",
    "            if best_emission_tag is not None:\n",
    "                updated_tags.append(best_emission_tag)\n",
    "            else:\n",
    "                # If no valid emission tag found, keep the original \"NULL\" tag\n",
    "                updated_tags.append(viterbi_tags[i])\n",
    "        else:\n",
    "            updated_tags.append(viterbi_tags[i])\n",
    "    \n",
    "    return updated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 25\n",
      "\n",
      "#Correct Entity : 11\n",
      "Entity  precision: 0.4400\n",
      "Entity  recall: 0.0480\n",
      "Entity  F: 0.0866\n",
      "\n",
      "#Correct Sentiment : 10\n",
      "Sentiment  precision: 0.4000\n",
      "Sentiment  recall: 0.0437\n",
      "Sentiment  F: 0.0787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "es_emission_dictionary, es_unique_tags = create_emission_dictionary(es_token_seq, es_tag_seq, 1)\n",
    "es_test_token_seq = read_test(\"ES/dev.in\")   \n",
    "es_transition_dictionary = create_transition_dictionary(es_tag_seq, es_unique_tags)\n",
    "es_predicted_tags_viterbi = viterbi_loop(es_test_token_seq, es_unique_tags, es_emission_dictionary, es_transition_dictionary)\n",
    "writeoutput(\"ES/dev.p2.out\", es_test_token_seq, es_predicted_tags_viterbi)\n",
    "\n",
    "\n",
    "!python \"evalResult.py\" \"ES/dev.out\" \"ES/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 123\n",
      "\n",
      "#Correct Entity : 56\n",
      "Entity  precision: 0.4553\n",
      "Entity  recall: 0.1440\n",
      "Entity  F: 0.2187\n",
      "\n",
      "#Correct Sentiment : 43\n",
      "Sentiment  precision: 0.3496\n",
      "Sentiment  recall: 0.1105\n",
      "Sentiment  F: 0.1680\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")   \n",
    "ru_transition_dictionary = create_transition_dictionary(ru_tag_seq, ru_unique_tags)\n",
    "ru_predicted_tags_viterbi = viterbi_loop(ru_test_token_seq, ru_unique_tags, ru_emission_dictionary, ru_transition_dictionary)\n",
    "writeoutput(\"RU/dev.p2.out\", ru_test_token_seq, ru_predicted_tags_viterbi)\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p2.out\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 \n",
    "Our approach is to train a new pytorch model \n",
    "We set 100 to the embedding dimension to have a better range of sensitivity to the relationships between each possible node\n",
    "and set 200 to the hidden dimension for a 200 layer RNN\n",
    "\n",
    "We set epoch to be 800 as it starts plateauing thereafter and batchsize to be 1000 to not be too big or too small for a stable gradient that is still sensitive to changes between dataset.\n",
    "\n",
    "It is trained using gpu on google collab link below\n",
    "\n",
    "https://colab.research.google.com/drive/1dsjMv1zOb1ruNEn0AfuFfSgZ1Kv1sYSo?usp=sharing\n",
    "\n",
    "We then loaded each test.in for RU and ES.\n",
    "\n",
    "To evaluate and generate the tags use the model russian_final.pt and espanol_final.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat to a df\n",
    "import pandas as pd\n",
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "\n",
    "ru_token_seq = pd.DataFrame(ru_token_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained using google collab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Sentimenttagging(Dataset):\n",
    "    def __init__(self, word_sequences, tag_sequences):\n",
    "        self.word_sequences = word_sequences\n",
    "        self.tag_sequences = tag_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = torch.LongTensor(self.word_sequences[idx])\n",
    "        tags = torch.LongTensor(self.tag_sequences[idx])\n",
    "        return words, tags\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Sort batch by sequence length (descending)\n",
    "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        words, tags = zip(*batch)\n",
    "\n",
    "        # Pad sequences\n",
    "        words_padded = nn.utils.rnn.pad_sequence(words, batch_first=True)\n",
    "        tags_padded = nn.utils.rnn.pad_sequence(tags, batch_first=True)\n",
    "\n",
    "        return words_padded, tags_padded\n",
    "    \n",
    "#  Inititate Torch Model\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim, hidden_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 1/1, Loss: 1.416395664215088\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzU0lEQVR4nO3df1iVVb7//9eGjYAICCoKCepYimYq6ckUzUHLpKIsu0pJRU9dWmFjWVcDJxXN1DIrZ8YfncwfY5k2mTKesjLHzJ9lpHick6UoKimU5cgPLVRY3z/6tL+zAxUU2GzX83Fd93V5r3vda7/Xwtov7/veG4cxxggAAMAiPp4uAAAAoK4RgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAJyXw+Go0rZx48bLep3JkyfL4XBc0rkbN26skRou57VXrlxZ568N4PI4PV0AgPpr+/btbvtTp07VJ598og0bNri1d+zY8bJe56GHHtLAgQMv6dzrr79e27dvv+waANiFAATgvG688Ua3/WbNmsnHx6dC+2+dPn1aDRs2rPLrtGzZUi1btrykGkNCQi5aDwD8FrfAAFyW3//+9+rUqZM2bdqkXr16qWHDhvrP//xPSdLbb7+tAQMGKDIyUoGBgerQoYPS0tJ06tQptzEquwXWunVr3XHHHfrwww91/fXXKzAwULGxsVq0aJFbv8pugY0cOVKNGjVSTk6ObrvtNjVq1EjR0dF68sknVVpa6nb+t99+q3vvvVfBwcFq3LixHnjgAX3xxRdyOBxasmRJjazRP//5T911110KCwtTQECAunbtqr/+9a9ufcrLy/Xcc8+pffv2CgwMVOPGjdW5c2f96U9/cvU5fvy4Ro8erejoaPn7+6tZs2aKj4/X+vXra6ROwCZcAQJw2fLz8zVs2DA9/fTTmj59unx8fvm31f79+3Xbbbfp8ccfV1BQkL7++mu98MIL2rFjR4XbaJXZvXu3nnzySaWlpal58+Z6/fXX9eCDD+rqq6/WTTfddMFzz549qzvvvFMPPvignnzySW3atElTp05VaGioJk2aJEk6deqUEhISdOLECb3wwgu6+uqr9eGHH+r++++//EX5f7755hv16tVLERER+vOf/6wmTZrozTff1MiRI/Xdd9/p6aefliTNnDlTkydP1oQJE3TTTTfp7Nmz+vrrr3Xy5EnXWMOHD9fOnTs1bdo0tWvXTidPntTOnTv1448/1li9gDUMAFRRSkqKCQoKcmvr27evkWT+8Y9/XPDc8vJyc/bsWfPpp58aSWb37t2uYxkZGea3/ztq1aqVCQgIMIcPH3a1/fTTTyY8PNyMGTPG1fbJJ58YSeaTTz5xq1OS+dvf/uY25m233Wbat2/v2p87d66RZD744AO3fmPGjDGSzOLFiy84p19f+5133jlvnyFDhhh/f39z5MgRt/bExETTsGFDc/LkSWOMMXfccYfp2rXrBV+vUaNG5vHHH79gHwBVwy0wAJctLCxM/fr1q9B+8OBBJScnq0WLFvL19ZWfn5/69u0rSdq7d+9Fx+3atatiYmJc+wEBAWrXrp0OHz580XMdDoeSkpLc2jp37ux27qeffqrg4OAKD2APHTr0ouNX1YYNG9S/f39FR0e7tY8cOVKnT592PWh+ww03aPfu3Xr00Uf10UcfqaioqMJYN9xwg5YsWaLnnntOn332mc6ePVtjdQK2IQABuGyRkZEV2kpKStSnTx99/vnneu6557Rx40Z98cUXWrVqlSTpp59+uui4TZo0qdDm7+9fpXMbNmyogICACuf+/PPPrv0ff/xRzZs3r3BuZW2X6scff6x0faKiolzHJSk9PV2zZs3SZ599psTERDVp0kT9+/dXVlaW65y3335bKSkpev3119WzZ0+Fh4drxIgRKigoqLF6AVsQgABctsq+w2fDhg06duyYFi1apIceekg33XSTunfvruDgYA9UWLkmTZrou+++q9Bek4GiSZMmys/Pr9B+7NgxSVLTpk0lSU6nU+PHj9fOnTt14sQJLV++XHl5ebr11lt1+vRpV9/Zs2fr0KFDOnz4sGbMmKFVq1Zp5MiRNVYvYAsCEIBa8Wso8vf3d2v/7//+b0+UU6m+ffuquLhYH3zwgVv7ihUrauw1+vfv7wqD/27p0qVq2LBhpR/hb9y4se69916lpqbqxIkTOnToUIU+MTExGjt2rG655Rbt3LmzxuoFbMGnwADUil69eiksLEwPP/ywMjIy5Ofnp2XLlmn37t2eLs0lJSVFr7zyioYNG6bnnntOV199tT744AN99NFHkuT6NNvFfPbZZ5W29+3bVxkZGXrvvfeUkJCgSZMmKTw8XMuWLdP777+vmTNnKjQ0VJKUlJSkTp06qXv37mrWrJkOHz6s2bNnq1WrVrrmmmtUWFiohIQEJScnKzY2VsHBwfriiy/04Ycf6p577qmZBQEsQgACUCuaNGmi999/X08++aSGDRumoKAg3XXXXXr77bd1/fXXe7o8SVJQUJA2bNigxx9/XE8//bQcDocGDBigefPm6bbbblPjxo2rNM5LL71Uafsnn3yi3//+99q2bZv+67/+S6mpqfrpp5/UoUMHLV682O3WVUJCgt599129/vrrKioqUosWLXTLLbdo4sSJ8vPzU0BAgHr06KE33nhDhw4d0tmzZxUTE6M//vGPro/SA6g6hzHGeLoIAKhPpk+frgkTJujIkSOX/A3VAOo3rgABsNqcOXMkSbGxsTp79qw2bNigP//5zxo2bBjhB7iCEYAAWK1hw4Z65ZVXdOjQIZWWlrpuK02YMMHTpQGoRdwCAwAA1uFj8AAAwDoEIAAAYB0CEAAAsA4PQVeivLxcx44dU3BwcKVf8Q8AAOofY4yKi4sVFRV10S8yJQBV4tixYxV+czMAAPAOeXl5F/0aCwJQJX79ZY15eXkKCQnxcDUAAKAqioqKFB0dXaVfukwAqsSvt71CQkIIQAAAeJmqPL7CQ9AAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALCORwPQpk2blJSUpKioKDkcDmVmZlb53K1bt8rpdKpr164Vjs2ePVvt27dXYGCgoqOj9cQTT+jnn3+uucIBAIBX82gAOnXqlLp06aI5c+ZU67zCwkKNGDFC/fv3r3Bs2bJlSktLU0ZGhvbu3auFCxfq7bffVnp6ek2VDQAAvJzTky+emJioxMTEap83ZswYJScny9fXt8JVo+3btys+Pl7JycmSpNatW2vo0KHasWNHTZQMAACuAF73DNDixYt14MABZWRkVHq8d+/e+vLLL12B5+DBg1q7dq1uv/32845ZWlqqoqIitw0AAFy5PHoFqLr279+vtLQ0bd68WU5n5aUPGTJEx48fV+/evWWM0blz5/TII48oLS3tvOPOmDFDU6ZMqa2yAQBAPeM1V4DKysqUnJysKVOmqF27duftt3HjRk2bNk3z5s3Tzp07tWrVKr333nuaOnXqec9JT09XYWGha8vLy6uNKQAAgHrCYYwxni5CkhwOh1avXq1BgwZVevzkyZMKCwuTr6+vq628vFzGGPn6+mrdunXq16+f+vTpoxtvvFEvvviiq9+bb76p0aNHq6SkRD4+F898RUVFCg0NVWFhoUJCQi57bgAAoPZV5/3ba26BhYSEaM+ePW5t8+bN04YNG7Ry5Uq1adNGknT69OkKIcfX11fGGNWTrAcAADzMowGopKREOTk5rv3c3FxlZ2crPDxcMTExSk9P19GjR7V06VL5+PioU6dObudHREQoICDArT0pKUkvv/yy4uLi1KNHD+Xk5GjixIm688473a4eAQAAe3k0AGVlZSkhIcG1P378eElSSkqKlixZovz8fB05cqRaY06YMEEOh0MTJkzQ0aNH1axZMyUlJWnatGk1WjsAAPBe9eYZoPqEZ4AAAPA+1Xn/9ppPgQEAANQUAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArOPRALRp0yYlJSUpKipKDodDmZmZVT5369atcjqd6tq1a4VjJ0+eVGpqqiIjIxUQEKAOHTpo7dq1NVc4AADwak5PvvipU6fUpUsXjRo1SoMHD67yeYWFhRoxYoT69++v7777zu3YmTNndMsttygiIkIrV65Uy5YtlZeXp+Dg4JouHwAAeCmPBqDExEQlJiZW+7wxY8YoOTlZvr6+Fa4aLVq0SCdOnNC2bdvk5+cnSWrVqlVNlAsAAK4QXvcM0OLFi3XgwAFlZGRUenzNmjXq2bOnUlNT1bx5c3Xq1EnTp09XWVnZeccsLS1VUVGR2wYAAK5cXhWA9u/fr7S0NC1btkxOZ+UXrw4ePKiVK1eqrKxMa9eu1YQJE/TSSy9p2rRp5x13xowZCg0NdW3R0dG1NQUAAFAPeE0AKisrU3JysqZMmaJ27dqdt195ebkiIiL02muvqVu3bhoyZIieeeYZzZ8//7znpKenq7Cw0LXl5eXVxhQAAEA94dFngKqjuLhYWVlZ2rVrl8aOHSvpl7BjjJHT6dS6devUr18/RUZGys/PT76+vq5zO3TooIKCAp05c0YNGjSoMLa/v7/8/f3rbC4AAMCzvCYAhYSEaM+ePW5t8+bN04YNG7Ry5Uq1adNGkhQfH6+33npL5eXl8vH55QLXvn37FBkZWWn4AQAA9vFoACopKVFOTo5rPzc3V9nZ2QoPD1dMTIzS09N19OhRLV26VD4+PurUqZPb+REREQoICHBrf+SRR/SXv/xF48aN02OPPab9+/dr+vTp+sMf/lBn8wIAAPWbRwNQVlaWEhISXPvjx4+XJKWkpGjJkiXKz8/XkSNHqjVmdHS01q1bpyeeeEKdO3fWVVddpXHjxumPf/xjjdYOAAC8l8MYYzxdRH1TVFSk0NBQFRYWKiQkxNPlAACAKqjO+7fXfAoMAACgphCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALCORwPQpk2blJSUpKioKDkcDmVmZlb53K1bt8rpdKpr167n7bNixQo5HA4NGjTosmsFAABXDo8GoFOnTqlLly6aM2dOtc4rLCzUiBEj1L9///P2OXz4sJ566in16dPncssEAABXGKcnXzwxMVGJiYnVPm/MmDFKTk6Wr69vpVeNysrK9MADD2jKlCnavHmzTp48efnFAgCAK4bXPQO0ePFiHThwQBkZGeft8+yzz6pZs2Z68MEHqzRmaWmpioqK3DYAAHDl8ugVoOrav3+/0tLStHnzZjmdlZe+detWLVy4UNnZ2VUed8aMGZoyZUoNVQkAAOo7r7kCVFZWpuTkZE2ZMkXt2rWrtE9xcbGGDRumBQsWqGnTplUeOz09XYWFha4tLy+vpsoGAAD1kNdcASouLlZWVpZ27dqlsWPHSpLKy8tljJHT6dS6desUHh6uQ4cOKSkpyXVeeXm5JMnpdOqbb75R27ZtK4zt7+8vf3//upkIAADwOK8JQCEhIdqzZ49b27x587RhwwatXLlSbdq0ka+vb4U+EyZMUHFxsf70pz8pOjq6LksGAAD1lEcDUElJiXJyclz7ubm5ys7OVnh4uGJiYpSenq6jR49q6dKl8vHxUadOndzOj4iIUEBAgFv7b/s0bty40nYAAGAvjwagrKwsJSQkuPbHjx8vSUpJSdGSJUuUn5+vI0eOeKo8AABwhXIYY4yni6hvioqKFBoaqsLCQoWEhHi6HAAAUAXVef/2mk+BAQAA1BQCEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACs49EAtGnTJiUlJSkqKkoOh0OZmZlVPnfr1q1yOp3q2rWrW/uCBQvUp08fhYWFKSwsTDfffLN27NhRs4UDAACvdkkBKC8vT99++61rf8eOHXr88cf12muvVWucU6dOqUuXLpozZ061zissLNSIESPUv3//Csc2btyooUOH6pNPPtH27dsVExOjAQMG6OjRo9V6DQAAcOVyGGNMdU/q06ePRo8ereHDh6ugoEDt27fXtddeq3379ukPf/iDJk2aVP1CHA6tXr1agwYNumjfIUOG6JprrpGvr68yMzOVnZ193r5lZWUKCwvTnDlzNGLEiCrVUlRUpNDQUBUWFiokJKSKMwAAAJ5UnffvS7oC9M9//lM33HCDJOlvf/ubOnXqpG3btumtt97SkiVLLmXIKlu8eLEOHDigjIyMKvU/ffq0zp49q/Dw8PP2KS0tVVFRkdsGAACuXJcUgM6ePSt/f39J0vr163XnnXdKkmJjY5Wfn19z1f3G/v37lZaWpmXLlsnpdFbpnLS0NF111VW6+eabz9tnxowZCg0NdW3R0dE1VTIAAKiHLikAXXvttXr11Ve1efNmffzxxxo4cKAk6dixY2rSpEmNFvirsrIyJScna8qUKWrXrl2Vzpk5c6aWL1+uVatWKSAg4Lz90tPTVVhY6Nry8vJqqmwAAFAPVe0yym+88MILuvvuu/Xiiy8qJSVFXbp0kSStWbPGdWusphUXFysrK0u7du3S2LFjJUnl5eUyxsjpdGrdunXq16+fq/+sWbM0ffp0rV+/Xp07d77g2P7+/q4rWgAA4Mp3SQHo97//vX744QcVFRUpLCzM1T569Gg1bNiwxor7dyEhIdqzZ49b27x587RhwwatXLlSbdq0cbW/+OKLeu655/TRRx+pe/futVIPAADwXpcUgH766ScZY1zh5/Dhw1q9erU6dOigW2+9tcrjlJSUKCcnx7Wfm5ur7OxshYeHKyYmRunp6Tp69KiWLl0qHx8fderUye38iIgIBQQEuLXPnDlTEydO1FtvvaXWrVuroKBAktSoUSM1atToUqYLAACuMJf0DNBdd92lpUuXSpJOnjypHj166KWXXtKgQYM0f/78Ko+TlZWluLg4xcXFSZLGjx+vuLg418fo8/PzdeTIkWrVNm/ePJ05c0b33nuvIiMjXdusWbOqNQ4AALhyXdL3ADVt2lSffvqprr32Wr3++uv6y1/+ol27dundd9/VpEmTtHfv3tqotc7wPUAAAHifWv8eoNOnTys4OFiStG7dOt1zzz3y8fHRjTfeqMOHD1/KkAAAAHXmkgLQ1VdfrczMTOXl5emjjz7SgAEDJEnff/89V0wAAEC9d0kBaNKkSXrqqafUunVr3XDDDerZs6ekX64G/fo8DwAAQH11Sc8ASVJBQYHy8/PVpUsX+fj8kqN27NihkJAQxcbG1miRdY1ngAAA8D7Vef++pI/BS1KLFi3UokULffvtt3I4HLrqqqtq7UsQAQAAatIl3QIrLy/Xs88+q9DQULVq1UoxMTFq3Lixpk6dqvLy8pquEQAAoEZd0hWgZ555RgsXLtTzzz+v+Ph4GWO0detWTZ48WT///LOmTZtW03UCAADUmEt6BigqKkqvvvqq67fA/+rvf/+7Hn30UR09erTGCvQEngECAMD71Pr3AJ04caLSB51jY2N14sSJSxkSAACgzlxSAOrSpYvmzJlToX3OnDkX/c3rAAAAnnZJzwDNnDlTt99+u9avX6+ePXvK4XBo27ZtysvL09q1a2u6RgAAgBp1SVeA+vbtq3379unuu+/WyZMndeLECd1zzz36v//7Py1evLimawQAAKhRl/xFiJXZvXu3rr/+epWVldXUkB7BQ9AAAHifWn8IGgAAwJsRgAAAgHUIQAAAwDrV+hTYPffcc8HjJ0+evJxaAAAA6kS1AlBoaOhFj48YMeKyCgIAAKht1QpAfMQdAABcCXgGCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKzj0QC0adMmJSUlKSoqSg6HQ5mZmVU+d+vWrXI6neratWuFY++++646duwof39/dezYUatXr665ogEAgNfzaAA6deqUunTpojlz5lTrvMLCQo0YMUL9+/evcGz79u26//77NXz4cO3evVvDhw/Xfffdp88//7ymygYAAF7OYYwxni5CkhwOh1avXq1BgwZdtO+QIUN0zTXXyNfXV5mZmcrOznYdu//++1VUVKQPPvjA1TZw4ECFhYVp+fLlVaqlqKhIoaGhKiwsVEhISHWnAgAAPKA6799e9wzQ4sWLdeDAAWVkZFR6fPv27RowYIBb26233qpt27add8zS0lIVFRW5bQAA4MrlVQFo//79SktL07Jly+R0OivtU1BQoObNm7u1NW/eXAUFBecdd8aMGQoNDXVt0dHRNVo3AACoX7wmAJWVlSk5OVlTpkxRu3btLtjX4XC47RtjKrT9u/T0dBUWFrq2vLy8GqkZAADUT5VfRqmHiouLlZWVpV27dmns2LGSpPLychlj5HQ6tW7dOvXr108tWrSocLXn+++/r3BV6N/5+/vL39+/VusHAAD1h9dcAQoJCdGePXuUnZ3t2h5++GG1b99e2dnZ6tGjhySpZ8+e+vjjj93OXbdunXr16uWJsgEAQD3k0StAJSUlysnJce3n5uYqOztb4eHhiomJUXp6uo4ePaqlS5fKx8dHnTp1cjs/IiJCAQEBbu3jxo3TTTfdpBdeeEF33XWX/v73v2v9+vXasmVLnc0LAADUbx69ApSVlaW4uDjFxcVJksaPH6+4uDhNmjRJkpSfn68jR45Ua8xevXppxYoVWrx4sTp37qwlS5bo7bffdl0hAgAAqDffA1Sf8D1AAAB4nyv6e4AAAAAuFwEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANbxaADatGmTkpKSFBUVJYfDoczMzAv237Jli+Lj49WkSRMFBgYqNjZWr7zySoV+s2fPVvv27RUYGKjo6Gg98cQT+vnnn2tpFgAAwNs4Pfnip06dUpcuXTRq1CgNHjz4ov2DgoI0duxYde7cWUFBQdqyZYvGjBmjoKAgjR49WpK0bNkypaWladGiRerVq5f27dunkSNHSlKlYQkAANjHowEoMTFRiYmJVe4fFxenuLg4137r1q21atUqbd682RWAtm/frvj4eCUnJ7v6DB06VDt27KjZ4gEAgNfy6meAdu3apW3btqlv376utt69e+vLL790BZ6DBw9q7dq1uv322887TmlpqYqKitw2AABw5fLoFaBL1bJlSx0/flznzp3T5MmT9dBDD7mODRkyRMePH1fv3r1ljNG5c+f0yCOPKC0t7bzjzZgxQ1OmTKmL0gEAQD3glVeANm/erKysLL366quaPXu2li9f7jq2ceNGTZs2TfPmzdPOnTu1atUqvffee5o6dep5x0tPT1dhYaFry8vLq4tpAAAAD/HKK0Bt2rSRJF133XX67rvvNHnyZA0dOlSSNHHiRA0fPtx1Vei6667TqVOnNHr0aD3zzDPy8amY+fz9/eXv7193EwAAAB7llVeA/p0xRqWlpa7906dPVwg5vr6+MsbIGFPX5QEAgHrIo1eASkpKlJOT49rPzc1Vdna2wsPDFRMTo/T0dB09elRLly6VJM2dO1cxMTGKjY2V9Mv3As2aNUuPPfaYa4ykpCS9/PLLiouLU48ePZSTk6OJEyfqzjvvlK+vb91OEAAA1EseDUBZWVlKSEhw7Y8fP16SlJKSoiVLlig/P19HjhxxHS8vL1d6erpyc3PldDrVtm1bPf/88xozZoyrz4QJE+RwODRhwgQdPXpUzZo1U1JSkqZNm1Z3EwMAAPWaw3BfqIKioiKFhoaqsLBQISEhni4HAABUQXXev73+GSAAAIDqIgABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoeDUCbNm1SUlKSoqKi5HA4lJmZecH+W7ZsUXx8vJo0aaLAwEDFxsbqlVdeqdDv5MmTSk1NVWRkpAICAtShQwetXbu2lmYBAAC8jdOTL37q1Cl16dJFo0aN0uDBgy/aPygoSGPHjlXnzp0VFBSkLVu2aMyYMQoKCtLo0aMlSWfOnNEtt9yiiIgIrVy5Ui1btlReXp6Cg4NrezoAAMBLeDQAJSYmKjExscr94+LiFBcX59pv3bq1Vq1apc2bN7sC0KJFi3TixAlt27ZNfn5+kqRWrVrVbOEAAMCrefUzQLt27dK2bdvUt29fV9uaNWvUs2dPpaamqnnz5urUqZOmT5+usrIyD1YKAADqE49eAbpULVu21PHjx3Xu3DlNnjxZDz30kOvYwYMHtWHDBj3wwANau3at9u/fr9TUVJ07d06TJk2qdLzS0lKVlpa69ouKimp9DgAAwHO8MgBt3rxZJSUl+uyzz5SWlqarr75aQ4cOlSSVl5crIiJCr732mnx9fdWtWzcdO3ZML7744nkD0IwZMzRlypS6nAIAAPAgrwxAbdq0kSRdd911+u677zR58mRXAIqMjJSfn598fX1d/Tt06KCCggKdOXNGDRo0qDBeenq6xo8f79ovKipSdHR0Lc8CAAB4ilc/AyRJxhi321fx8fHKyclReXm5q23fvn2KjIysNPxIkr+/v0JCQtw2AABw5fJoACopKVF2drays7MlSbm5ucrOztaRI0ck/XJlZsSIEa7+c+fO1f/8z/9o//792r9/vxYvXqxZs2Zp2LBhrj6PPPKIfvzxR40bN0779u3T+++/r+nTpys1NbVO5wYAAOovj94Cy8rKUkJCgmv/19tQKSkpWrJkifLz811hSPrl+Z709HTl5ubK6XSqbdu2ev755zVmzBhXn+joaK1bt05PPPGEOnfurKuuukrjxo3TH//4x7qbGAAAqNccxhjj6SLqm6KiIoWGhqqwsJDbYQAAeInqvH97/TNAAAAA1UUAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6Hg1AmzZtUlJSkqKiouRwOJSZmXnB/lu2bFF8fLyaNGmiwMBAxcbG6pVXXjlv/xUrVsjhcGjQoEE1WzgAAPBqTk+++KlTp9SlSxeNGjVKgwcPvmj/oKAgjR07Vp07d1ZQUJC2bNmiMWPGKCgoSKNHj3bre/jwYT311FPq06dPbZUPAAC8lMMYYzxdhCQ5HA6tXr262ldr7rnnHgUFBemNN95wtZWVlalv374aNWqUNm/erJMnT1706tK/KyoqUmhoqAoLCxUSElKtegAAgGdU5/3bq58B2rVrl7Zt26a+ffu6tT/77LNq1qyZHnzwwSqNU1paqqKiIrcNAABcuTx6C+xStWzZUsePH9e5c+c0efJkPfTQQ65jW7du1cKFC5WdnV3l8WbMmKEpU6bUQqUAAKA+8sorQJs3b1ZWVpZeffVVzZ49W8uXL5ckFRcXa9iwYVqwYIGaNm1a5fHS09NVWFjo2vLy8mqrdAAAUA945RWgNm3aSJKuu+46fffdd5o8ebKGDh2qAwcO6NChQ0pKSnL1LS8vlyQ5nU598803atu2bYXx/P395e/vXzfFAwAAj/PKAPTvjDEqLS2VJMXGxmrPnj1uxydMmKDi4mL96U9/UnR0dJXHlMSzQAAAeJFf37er8vkujwagkpIS5eTkuPZzc3OVnZ2t8PBwxcTEKD09XUePHtXSpUslSXPnzlVMTIxiY2Ml/fK9QLNmzdJjjz0mSQoICFCnTp3cXqNx48aSVKH9QoqLiyWpyoEJAADUH8XFxQoNDb1gH48GoKysLCUkJLj2x48fL0lKSUnRkiVLlJ+fryNHjriOl5eXKz09Xbm5uXI6nWrbtq2ef/55jRkzpkbrioqKUl5enoKDg+VwOGp0bG9UVFSk6Oho5eXl8bUAtYh1rhusc91gnesOa/3/M8aouLhYUVFRF+1bb74HCPUX34tUN1jnusE61w3Wue6w1pfGKz8FBgAAcDkIQAAAwDoEIFyUv7+/MjIy+KqAWsY61w3WuW6wznWHtb40PAMEAACswxUgAABgHQIQAACwDgEIAABYhwAEAACsQwCy0Lx589SmTRsFBASoW7du2rx58wX7z507Vx06dFBgYKDat2/v+tUk/+7kyZNKTU1VZGSkAgIC1KFDB61du7a2puAVamOdZ8+erfbt2yswMFDR0dF64okn9PPPP9fWFOq9TZs2KSkpSVFRUXI4HMrMzLzoOZ9++qm6deumgIAA/e53v9Orr75aoc+7776rjh07yt/fXx07dtTq1atroXrvUhtrvWDBAvXp00dhYWEKCwvTzTffrB07dtTSDLxDbf2d/tWKFSvkcDg0aNCgmivaWxlYZcWKFcbPz88sWLDAfPXVV2bcuHEmKCjIHD58uNL+8+bNM8HBwWbFihXmwIEDZvny5aZRo0ZmzZo1rj6lpaWme/fu5rbbbjNbtmwxhw4dMps3bzbZ2dl1Na16pzbW+c033zT+/v5m2bJlJjc313z00UcmMjLSPP7443U1rXpn7dq15plnnjHvvvuukWRWr159wf4HDx40DRs2NOPGjTNfffWVWbBggfHz8zMrV6509dm2bZvx9fU106dPN3v37jXTp083TqfTfPbZZ7U8m/qtNtY6OTnZzJ071+zatcvs3bvXjBo1yoSGhppvv/22lmdTf9XGOv/q0KFD5qqrrjJ9+vQxd911V+1MwIsQgCxzww03mIcfftitLTY21qSlpVXav2fPnuapp55yaxs3bpyJj4937c+fP9/87ne/M2fOnKn5gr1Ubaxzamqq6devn1uf8ePHm969e9dQ1d6tKm8WTz/9tImNjXVrGzNmjLnxxhtd+/fdd58ZOHCgW59bb73VDBkypMZq9XY1tda/de7cORMcHGz++te/1kSZXq8m1/ncuXMmPj7evP766yYlJYUAZIzhFphFzpw5oy+//FIDBgxwax8wYIC2bdtW6TmlpaUKCAhwawsMDNSOHTt09uxZSdKaNWvUs2dPpaamqnnz5urUqZOmT5+usrKy2plIPVdb69y7d299+eWXrlsEBw8e1Nq1a3X77bfXwiyuTNu3b6/wc7n11luVlZXlWufz9Tnfzw6Vq8pa/9bp06d19uxZhYeH10WJV4SqrvOzzz6rZs2a6cEHH6zrEustApBFfvjhB5WVlal58+Zu7c2bN1dBQUGl59x66616/fXX9eWXX8oYo6ysLC1atEhnz57VDz/8IOmXN+KVK1eqrKxMa9eu1YQJE/TSSy9p2rRptT6n+qi21nnIkCGaOnWqevfuLT8/P7Vt21YJCQlKS0ur9TldKQoKCir9uZw7d861zufrc76fHSpXlbX+rbS0NF111VW6+eab66LEK0JV1nnr1q1auHChFixY4IkS6y2npwtA3XM4HG77xpgKbb+aOHGiCgoKdOONN8oYo+bNm2vkyJGaOXOmfH19JUnl5eWKiIjQa6+9Jl9fX3Xr1k3Hjh3Tiy++qEmTJtX6fOqrml7njRs3atq0aZo3b5569OihnJwcjRs3TpGRkZo4cWKtz+dKUdnP5bft1fnZ4fyqsta/mjlzppYvX66NGzdWuBqKC7vQOhcXF2vYsGFasGCBmjZt6ony6i2uAFmkadOm8vX1rfAv2e+//77CvyB+FRgYqEWLFun06dM6dOiQjhw5otatWys4ONj1H1NkZKTatWvneqOWpA4dOqigoEBnzpypvQnVU7W1zhMnTtTw4cP10EMP6brrrtPdd9+t6dOna8aMGSovL6/1eV0JWrRoUenPxel0qkmTJhfsc76fHSpXlbX+1axZszR9+nStW7dOnTt3rssyvd7F1vnAgQM6dOiQkpKS5HQ65XQ6tXTpUq1Zs0ZOp1MHDhzwUOWeRwCySIMGDdStWzd9/PHHbu0ff/yxevXqdcFz/fz81LJlS/n6+mrFihW644475OPzy1+f+Ph45eTkuL0J79u3T5GRkWrQoEHNT6Seq611Pn36tOvPv/L19ZX55cMMNTuJK1TPnj0r/FzWrVun7t27y8/P74J9Lvazg7uqrLUkvfjii5o6dao+/PBDde/eva7L9HoXW+fY2Fjt2bNH2dnZru3OO+9UQkKCsrOzFR0d7aHK6wHPPHsNT/n149kLFy40X331lXn88cdNUFCQOXTokDHGmLS0NDN8+HBX/2+++ca88cYbZt++febzzz83999/vwkPDze5ubmuPkeOHDGNGjUyY8eONd9884157733TEREhHnuuefqenr1Rm2sc0ZGhgkODjbLly83Bw8eNOvWrTNt27Y19913X11Pr94oLi42u3btMrt27TKSzMsvv2x27drl+rqB367zrx8ZfuKJJ8xXX31lFi5cWOEjw1u3bjW+vr7m+eefN3v37jXPP/88H4M3tbPWL7zwgmnQoIFZuXKlyc/Pd23FxcV1Pr/6ojbW+bf4FNgvCEAWmjt3rmnVqpVp0KCBuf76682nn37qOpaSkmL69u3r2v/qq69M165dTWBgoAkJCTF33XWX+frrryuMuW3bNtOjRw/j7+9vfve735lp06aZc+fO1cV06q2aXuezZ8+ayZMnm7Zt25qAgAATHR1tHn30UfOvf/2rjmZU/3zyySdGUoUtJSXFGFNxnY0xZuPGjSYuLs40aNDAtG7d2syfP7/CuO+8845p37698fPzM7Gxsebdd9+tg9nUb7Wx1q1atap0zIyMjLqZVD1UW3+n/x0B6BcOY7h2DgAA7MIzQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAOA8HA6HMjMzPV0GgFpAAAJQL40cOVIOh6PCNnDgQE+XBuAK4PR0AQBwPgMHDtTixYvd2vz9/T1UDYArCVeAANRb/v7+atGihdsWFhYm6ZfbU/Pnz1diYqICAwPVpk0bvfPOO27n79mzR/369VNgYKCaNGmi0aNHq6SkxK3PokWLdO2118rf31+RkZEaO3as2/EffvhBd999txo2bKhrrrlGa9ascR3717/+pQceeEDNmjVTYGCgrrnmmgqBDUD9RAAC4LUmTpyowYMHa/fu3Ro2bJiGDh2qvXv3SpJOnz6tgQMHKiwsTF988YXeeecdrV+/3i3gzJ8/X6mpqRo9erT27NmjNWvW6Oqrr3Z7jSlTpui+++7T//7v/+q2227TAw88oBMnTrhe/6uvvtIHH3ygvXv3av78+WratGndLQCAS+fp38YKAJVJSUkxvr6+JigoyG179tlnjTHGSDIPP/yw2zk9evQwjzzyiDHGmNdee82EhYWZkpIS1/H333/f+Pj4mIKCAmOMMVFRUeaZZ545bw2SzIQJE1z7JSUlxuFwmA8++MAYY0xSUpIZNWpUzUwYQJ3iGSAA9VZCQoLmz5/v1hYeHu76c8+ePd2O9ezZU9nZ2ZKkvXv3qkuXLgoKCnIdj4+PV3l5ub755hs5HA4dO3ZM/fv3v2ANnTt3dv05KChIwcHB+v777yVJjzzyiAYPHqydO3dqwIABGjRokHr16nVJcwVQtwhAAOqtoKCgCrekLsbhcEiSjDGuP1fWJzAwsErj+fn5VTi3vLxckpSYmKjDhw/r/fff1/r169W/f3+lpqZq1qxZ1aoZQN3jGSAAXuuzzz6rsB8bGytJ6tixo7Kzs3Xq1CnX8a1bt8rHx0ft2rVTcHCwWrdurX/84x+XVUOzZs00cuRIvfnmm5o9e7Zee+21yxoPQN3gChCAequ0tFQFBQVubU6n0/Wg8TvvvKPu3burd+/eWrZsmXbs2KGFCxdKkh544AFlZGQoJSVFkydP1vHjx/XYY49p+PDhat68uSRp8uTJevjhhxUREaHExEQVFxdr69ateuyxx6pU36RJk9StWzdde+21Ki0t1XvvvacOHTrU4AoAqC0EIAD11ocffqjIyEi3tvbt2+vrr7+W9MsntFasWKFHH31ULVq00LJly9SxY0dJUsOGDfXRRx9p3Lhx+o//+A81bNhQgwcP1ssvv+waKyUlRT///LNeeeUVPfXUU2ratKnuvffeKtfXoEEDpaen69ChQwoMDFSfPn20YsWKGpg5gNrmMMYYTxcBANXlcDi0evVqDRo0yNOlAPBCPAMEAACsQwACAADW4RkgAF6Ju/cALgdXgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdf4/sM4294SPfDEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DO NOT RUN\n",
    "    \n",
    "# Sample word_sequences = ['Еда', 'вкусная', ',', 'но', 'отдельно', 'хочу', 'отметить', 'красивую', 'сервировку', 'блюд', ';', '.']\n",
    "# Sample tag_sequences = ['B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'I-positive', 'O', 'O']\n",
    "    \n",
    "# Set ur file path for ur train sequence\n",
    "word_sequences , tag_sequences = read_train(\"ES/train\")\n",
    "\n",
    "# Word mappings and index\n",
    "word_to_idx = {}  \n",
    "tag_to_idx = {}   \n",
    "\n",
    "# Iterate through the word and tag sequences to populate the dictionaries\n",
    "for sentence in word_sequences:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for tags in tag_sequences:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_idx:\n",
    "            tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "word_sequences = [[word_to_idx[word] for word in seq] for seq in word_sequences]\n",
    "tag_sequences = [[tag_to_idx[tag] for tag in seq] for seq in tag_sequences]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "tag_size = len(tag_to_idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 200\n",
    "# Create the dataset and DataLoader\n",
    "dataset = Sentimenttagging(word_sequences, tag_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=1000, collate_fn=dataset.collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = SentimentModel(vocab_size, tag_size, embedding_dim, hidden_dim)\n",
    "model.to(device)  # Move model to GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for words, tags in dataloader:\n",
    "        # Convert words and tags to tensors\n",
    "        # words = torch.LongTensor(words)\n",
    "        # tags = torch.LongTensor(tags)\n",
    "        words = words.to(device)  \n",
    "        tags = tags.to(device)    \n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.view(-1, tag_size), tags.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "model_save_path = \"test.pt\" #set ur own\n",
    "torch.save(model.state_dict(), model_save_path)    \n",
    "plt.plot(range(1, epochs + 1), loss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload to check\n",
    "model_save_path = \"espanol_final.pt\"\n",
    "\n",
    "\n",
    "loaded_model = SentimentModel(vocab_size, tag_size, 100, 200)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def convert_text_to_indices(text, word_to_idx):\n",
    "    if type(text) == list :\n",
    "        words = text\n",
    "    else:\n",
    "        words = text.split()\n",
    "    default_idx = word_to_idx.get(\"O\", 0)  # Use index 0 as default for unknown words\n",
    "    indices = [word_to_idx.get(word, default_idx) for word in words]\n",
    "    return indices\n",
    "\n",
    "def predict_text_sequence(model, text, word_to_idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        indices = convert_text_to_indices(text, word_to_idx)\n",
    "        inputs = torch.LongTensor(indices).unsqueeze(0)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        return predicted.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test/ES/test.in'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m token_seq\n\u001b[0;32m     18\u001b[0m idx_to_tag \u001b[39m=\u001b[39m {idx: tag \u001b[39mfor\u001b[39;00m tag, idx \u001b[39min\u001b[39;00m tag_to_idx\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m---> 20\u001b[0m ru_test_token \u001b[39m=\u001b[39m read_test(\u001b[39m\"\u001b[39;49m\u001b[39mtest/ES/test.in\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(ru_test_token))\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(ru_test_token)\n",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m, in \u001b[0;36mread_test\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_test\u001b[39m(filename):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         token_seq, current_token \u001b[39m=\u001b[39m [], []\n\u001b[0;32m      4\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n",
      "File \u001b[1;32mc:\\Users\\ozhen\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test/ES/test.in'"
     ]
    }
   ],
   "source": [
    "def read_test(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        token_seq, current_token = [], []\n",
    "        for line in file:\n",
    "            token = line.strip()  # Assuming each line is a token\n",
    "\n",
    "            if token:\n",
    "                current_token.append(token)\n",
    "            else:\n",
    "                if current_token:\n",
    "                    token_seq.append(current_token)\n",
    "                    current_token = []\n",
    "\n",
    "        if current_token:\n",
    "            token_seq.append(current_token)\n",
    "            \n",
    "    return token_seq\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "\n",
    "ru_test_token = read_test(\"ES/test.in\")\n",
    "print(len(ru_test_token))\n",
    "print(ru_test_token)\n",
    "label_list = []\n",
    "predictions=[]\n",
    "for index , each_sentence in enumerate(ru_test_token) :\n",
    "    print(each_sentence)\n",
    "    predictions = predict_text_sequence(loaded_model, each_sentence, word_to_idx)\n",
    "    if len(each_sentence) == 1:\n",
    "        predictions = [predictions]\n",
    "    print(predictions)\n",
    "    # Convert predicted indices to tag labels\n",
    "    predicted_tags = [idx_to_tag[idx] for idx in predictions]\n",
    "    print(len(each_sentence),len(predictions))\n",
    "    label_list.append(predicted_tags)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
