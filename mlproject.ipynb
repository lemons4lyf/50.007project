{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all my stuff\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        token_seq , tag_seq , current_token , current_tag = [],[],[],[]\n",
    "        for line in file:\n",
    "            token_tag = line.strip().split(\" \")\n",
    "\n",
    "            if len(token_tag) == 2:\n",
    "                current_token += [token_tag[0]]\n",
    "                current_tag += [token_tag[1]]\n",
    "            else:\n",
    "                token_seq += [current_token]\n",
    "                tag_seq += [current_tag]\n",
    "                current_token = []\n",
    "                current_tag = []\n",
    "        if (len(current_token) != 0):\n",
    "            token_seq += [current_token]\n",
    "            tag_seq += [current_tag]\n",
    "    return token_seq, tag_seq\n",
    "\n",
    "def read_test(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        token_seq , current_token = [],[]\n",
    "        for line in file:\n",
    "            token_test = line.strip()\n",
    "\n",
    "            if len(token_test) !=0:\n",
    "                current_token += [token_test]\n",
    "                \n",
    "            else:\n",
    "                token_seq += [current_token]\n",
    "                current_token = []  \n",
    "\n",
    "        if (len(current_token) != 0):\n",
    "            token_seq += [current_token]\n",
    "            \n",
    "    return token_seq\n",
    "\n",
    "def create_emission_dictionary(token_seq, tag_seq,k):\n",
    "    emission_dictionary = {} \n",
    "    count_tag = {} #number of words tagged with tag\n",
    "    count_token_tagged_tag = {} #number of times a token is tagged with tag\n",
    "    for i in range(0,len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not (y in count_tag.keys()):\n",
    "                count_tag[y] = 0\n",
    "\n",
    "            count_tag[y] += 1\n",
    "\n",
    "            if not (x in count_token_tagged_tag.keys()):\n",
    "                count_token_tagged_tag[x] = {}\n",
    "\n",
    "            if not (y in count_token_tagged_tag[x].keys()):\n",
    "                count_token_tagged_tag[x][y] = 0\n",
    "\n",
    "            count_token_tagged_tag[x][y] += 1\n",
    "    \n",
    "    for i in range(0, len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not(x in emission_dictionary.keys()):\n",
    "                emission_dictionary[x] = {}\n",
    "            emission_dictionary[x][y] = (count_token_tagged_tag[x][y]) / (count_tag[y] + k)\n",
    "            emission_dictionary[x][\"START\"] = 0\n",
    "            emission_dictionary[x][\"STOP\"] = 0\n",
    "\n",
    "    emission_dictionary[\"#UNK#\"] = {}\n",
    "    for tag in count_tag.keys():\n",
    "        emission_dictionary[\"#UNK#\"][tag] = k / (count_tag[tag] + k)\n",
    "    uniquetags = list(count_tag.keys())\n",
    "    return emission_dictionary , uniquetags\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if not (tag in uniquetags):\n",
    "        return 0\n",
    "    elif not (token in emission_dictionary.keys()):\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    else:\n",
    "        if not (tag in emission_dictionary[token].keys()):\n",
    "            emission_dictionary[token][tag] = 0\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "\n",
    "def simple_sentiment_analysis(test_token,emission_dictionary, uniquetags):\n",
    "    predicted_tags = []\n",
    "    for token_seq in test_token:\n",
    "        current_token = []\n",
    "        for token in token_seq:\n",
    "            predicted_tag = \"\"\n",
    "            max_prob = 0\n",
    "            for tag in uniquetags:\n",
    "                prob = emission(token, tag, emission_dictionary, uniquetags)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    predicted_tag = tag\n",
    "            current_token += [predicted_tag]\n",
    "        predicted_tags += [current_token]\n",
    "        current_token = []\n",
    "    return predicted_tags\n",
    "\n",
    "def writeoutput(filename, tokens, predicted_tags):\n",
    "  with open(filename, 'w', encoding='utf-8') as file:\n",
    "\n",
    "    for i in range(0,len(tokens)):\n",
    "      for j in range(0,len(tokens[i])):\n",
    "        file.write(tokens[i][j] + \" \" + predicted_tags[i][j] + \"\\n\" )\n",
    "\n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1466\n",
      "\n",
      "#Correct Entity : 178\n",
      "Entity  precision: 0.1214\n",
      "Entity  recall: 0.7773\n",
      "Entity  F: 0.2100\n",
      "\n",
      "#Correct Sentiment : 97\n",
      "Sentiment  precision: 0.0662\n",
      "Sentiment  recall: 0.4236\n",
      "Sentiment  F: 0.1145\n"
     ]
    }
   ],
   "source": [
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "es_emission_dictionary, es_unique_tags = create_emission_dictionary(es_token_seq, es_tag_seq, 1)\n",
    "es_test_token_seq = read_test(\"ES/dev.in\")\n",
    "\n",
    "es_predicted_tags = simple_sentiment_analysis(es_test_token_seq, es_emission_dictionary, es_unique_tags)\n",
    "\n",
    "writeoutput(\"ES/dev.p1.out\", es_test_token_seq, es_predicted_tags)\n",
    "\n",
    "!python \"evalResult.py\" \"ES/dev.out\" \"ES/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1816\n",
      "\n",
      "#Correct Entity : 266\n",
      "Entity  precision: 0.1465\n",
      "Entity  recall: 0.6838\n",
      "Entity  F: 0.2413\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.0710\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.1170\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")\n",
    "\n",
    "ru_predicted_tags = simple_sentiment_analysis(ru_test_token_seq, ru_emission_dictionary, ru_unique_tags)\n",
    "\n",
    "writeoutput(\"RU/dev.p1.out\", ru_test_token_seq, ru_predicted_tags)\n",
    "\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p1.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in my array of tags and return dictionary that gives trasnsition param\n",
    "\n",
    "def create_transition_dictionary(tag_seq, unique_tags):\n",
    "    transition_dictionary = {}\n",
    "\n",
    "    for first_tag in unique_tags:\n",
    "        for second_tag in unique_tags:\n",
    "            count, total = 0, 0\n",
    "            for i in tag_seq:\n",
    "                total += len(i) - 1\n",
    "                for j in range(len(i) - 1):\n",
    "                    if i[j] == first_tag and i[j+1] == second_tag:\n",
    "                        count += 1\n",
    "            if count != 0:\n",
    "                transition_dictionary[(first_tag, second_tag)] = count / total\n",
    "\n",
    "    start_dict = Counter(i[0] for i in tag_seq)\n",
    "    stop_dict = Counter(i[-1] for i in tag_seq)\n",
    "\n",
    "    total_sentences = len(tag_seq)\n",
    "\n",
    "    for tag, count in start_dict.items():\n",
    "        transition_dictionary[('START', tag)] = count / total_sentences\n",
    "\n",
    "    for tag, count in stop_dict.items():\n",
    "        transition_dictionary[(tag, 'STOP')] = count / total_sentences\n",
    "\n",
    "    return transition_dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(transition_dict, previous_tag, current_tag):\n",
    "    transition_key = (previous_tag, current_tag)\n",
    "    \n",
    "    if transition_key not in transition_dict:\n",
    "        transition_dict[transition_key] = math.log(1)\n",
    "    \n",
    "    return transition_dict[transition_key]\n",
    "\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if tag not in uniquetags:\n",
    "        return math.log(1)\n",
    "    elif token not in emission_dictionary:\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    elif tag not in emission_dictionary[token]:\n",
    "        return math.log(1)\n",
    "    else:\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "\n",
    "def viterbi(test_token_seq_sentence, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    viterbi_probs = {0: {\"START\": math.log(1)}, **{i: {tag: -math.inf for tag in unique_tags} for i in range(1, len(test_token_seq_sentence) + 1)}}\n",
    "    \n",
    "    # Calculate probabilities for each tag in the first word\n",
    "    for tag in unique_tags:\n",
    "        emission_prob = emission(test_token_seq_sentence[0], tag, emission_dictionary, unique_tags)\n",
    "        transition_prob = transition(transition_dictionary, \"START\", tag)\n",
    "        \n",
    "        if emission_prob == 0 or transition_prob == 0:\n",
    "            viterbi_probs[1][tag] = -math.inf\n",
    "        else:\n",
    "            viterbi_probs[1][tag] = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[0][\"START\"]\n",
    "            \n",
    "    # Calculate probabilities for each tag in the rest of the words\n",
    "    for i in range(1, len(test_token_seq_sentence)):\n",
    "        next_token = test_token_seq_sentence[i]\n",
    "        for j in unique_tags:\n",
    "            max_prob = -math.inf\n",
    "            for k in unique_tags:\n",
    "                emission_prob = emission(next_token, j, emission_dictionary, unique_tags)\n",
    "                transition_prob = transition(transition_dictionary, k, j)\n",
    "                \n",
    "                if emission_prob == 0 or transition_prob == 0:\n",
    "                    prob = -math.inf\n",
    "                else:\n",
    "                    prob = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[i][k]\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "            viterbi_probs[i+1][j] = max_prob\n",
    "    \n",
    "    # Calculate probabilities for transitioning to the \"STOP\" tag\n",
    "    max_prob = -math.inf\n",
    "    for tag in unique_tags:\n",
    "        previous_prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = transition(transition_dictionary, tag, \"STOP\")\n",
    "        \n",
    "        if transition_prob == 0:\n",
    "            prob = -math.inf\n",
    "        else:\n",
    "            prob = previous_prob + math.log(transition_prob)\n",
    "        \n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1] = {}\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1][\"STOP\"] = max_prob\n",
    "    \n",
    "    # Backtracking to find the best path\n",
    "    best_path = []\n",
    "    argmax = -math.inf\n",
    "    currentmax = -math.inf\n",
    "    argmax_tag = \"NULL\"\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = transition(transition_dictionary, tag, \"STOP\")\n",
    "        if transition_prob != 0:\n",
    "            currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "\n",
    "    best_path.append(argmax_tag)\n",
    "    \n",
    "    # Backpropagation\n",
    "    for i in range(len(test_token_seq_sentence), 1, -1):\n",
    "        argmax = -math.inf\n",
    "        currentmax = math.log(1)\n",
    "        \n",
    "        for tag in unique_tags:\n",
    "            prob = viterbi_probs[i-1][tag]\n",
    "            transition_prob = transition(transition_dictionary, tag, best_path[-1])\n",
    "            \n",
    "            if transition_prob == 0:\n",
    "                currentmax = -math.inf\n",
    "            else:\n",
    "                currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "        best_path.append(argmax_tag)\n",
    "\n",
    "    best_path.reverse()\n",
    "    return best_path\n",
    "\n",
    "def viterbi_loop(test_token_seq, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    result = []\n",
    "\n",
    "    for sentence in test_token_seq:\n",
    "        viterbi_tags = viterbi(sentence, unique_tags, emission_dictionary, transition_dictionary)\n",
    "        updated_tags = handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary)\n",
    "        result.append(updated_tags)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary):\n",
    "    updated_tags = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if viterbi_tags[i] == \"NULL\":\n",
    "            word = sentence[i]\n",
    "            max_emission_value = -math.inf\n",
    "            best_emission_tag = None\n",
    "\n",
    "            for tag in unique_tags:\n",
    "                emission_prob = emission(word, tag, emission_dictionary, unique_tags)\n",
    "                if emission_prob > max_emission_value:\n",
    "                    max_emission_value = emission_prob\n",
    "                    best_emission_tag = tag\n",
    "\n",
    "            if best_emission_tag is not None:\n",
    "                updated_tags.append(best_emission_tag)\n",
    "            else:\n",
    "                # If no valid emission tag found, keep the original \"NULL\" tag\n",
    "                updated_tags.append(viterbi_tags[i])\n",
    "        else:\n",
    "            updated_tags.append(viterbi_tags[i])\n",
    "    \n",
    "    return updated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 25\n",
      "\n",
      "#Correct Entity : 11\n",
      "Entity  precision: 0.4400\n",
      "Entity  recall: 0.0480\n",
      "Entity  F: 0.0866\n",
      "\n",
      "#Correct Sentiment : 10\n",
      "Sentiment  precision: 0.4000\n",
      "Sentiment  recall: 0.0437\n",
      "Sentiment  F: 0.0787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "es_emission_dictionary, es_unique_tags = create_emission_dictionary(es_token_seq, es_tag_seq, 1)\n",
    "es_test_token_seq = read_test(\"ES/dev.in\")   \n",
    "es_transition_dictionary = create_transition_dictionary(es_tag_seq, es_unique_tags)\n",
    "es_predicted_tags_viterbi = viterbi_loop(es_test_token_seq, es_unique_tags, es_emission_dictionary, es_transition_dictionary)\n",
    "writeoutput(\"ES/dev.p2.out\", es_test_token_seq, es_predicted_tags_viterbi)\n",
    "\n",
    "\n",
    "!python \"evalResult.py\" \"ES/dev.out\" \"ES/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 123\n",
      "\n",
      "#Correct Entity : 56\n",
      "Entity  precision: 0.4553\n",
      "Entity  recall: 0.1440\n",
      "Entity  F: 0.2187\n",
      "\n",
      "#Correct Sentiment : 43\n",
      "Sentiment  precision: 0.3496\n",
      "Sentiment  recall: 0.1105\n",
      "Sentiment  F: 0.1680\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")   \n",
    "ru_transition_dictionary = create_transition_dictionary(ru_tag_seq, ru_unique_tags)\n",
    "ru_predicted_tags_viterbi = viterbi_loop(ru_test_token_seq, ru_unique_tags, ru_emission_dictionary, ru_transition_dictionary)\n",
    "writeoutput(\"RU/dev.p2.out\", ru_test_token_seq, ru_predicted_tags_viterbi)\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p2.out\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(transition_dict, previous_tag, current_tag):\n",
    "    transition_key = (previous_tag, current_tag)\n",
    "    \n",
    "    if transition_key not in transition_dict:\n",
    "        transition_dict[transition_key] = math.log(1)\n",
    "    \n",
    "    return transition_dict[transition_key]\n",
    "\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if tag not in uniquetags:\n",
    "        return math.log(1)\n",
    "    elif token not in emission_dictionary:\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    elif tag not in emission_dictionary[token]:\n",
    "        return math.log(1)\n",
    "    else:\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "def handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary):\n",
    "    updated_tags = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if viterbi_tags[i] == \"NULL\":\n",
    "            word = sentence[i]\n",
    "            max_emission_value = -math.inf\n",
    "            best_emission_tag = None\n",
    "\n",
    "            for tag in unique_tags:\n",
    "                emission_prob = emission(word, tag, emission_dictionary, unique_tags)\n",
    "                if emission_prob > max_emission_value:\n",
    "                    max_emission_value = emission_prob\n",
    "                    best_emission_tag = tag\n",
    "\n",
    "            if best_emission_tag is not None:\n",
    "                updated_tags.append(best_emission_tag)\n",
    "            else:\n",
    "                # If no valid emission tag found, keep the original \"NULL\" tag\n",
    "                updated_tags.append(viterbi_tags[i])\n",
    "        else:\n",
    "            updated_tags.append(viterbi_tags[i])\n",
    "    \n",
    "    return updated_tags\n",
    "    \n",
    "def viterbi_k(test_token_seq_sentence, unique_tags, emission_dictionary, transition_dictionary, k=7):\n",
    "    viterbi_probs = {0: {\"START\": [(math.log(1), [])]}, **{i: {tag: [] for tag in unique_tags} for i in range(1, len(test_token_seq_sentence) + 1)}}\n",
    "    \n",
    "    # Calculate probabilities for each tag in the first word\n",
    "    for tag in unique_tags:\n",
    "        emission_prob = emission(test_token_seq_sentence[0], tag, emission_dictionary, unique_tags)\n",
    "        transition_prob = transition(transition_dictionary, \"START\", tag)\n",
    "        \n",
    "        if emission_prob != 0 and transition_prob != 0:\n",
    "            viterbi_probs[1][tag] = [(math.log(emission_prob) + math.log(transition_prob), [])]\n",
    "    \n",
    "    # Calculate probabilities for each tag in the rest of the words\n",
    "    for i in range(1, len(test_token_seq_sentence)):\n",
    "        next_token = test_token_seq_sentence[i]\n",
    "        for j in unique_tags:\n",
    "            max_k_paths = []\n",
    "            for k_prev in unique_tags:\n",
    "                if k_prev in viterbi_probs[i]:\n",
    "                    for prev_prob, prev_path in viterbi_probs[i][k_prev]:\n",
    "                        emission_prob = emission(next_token, j, emission_dictionary, unique_tags)\n",
    "                        transition_prob = transition(transition_dictionary, k_prev, j)\n",
    "                        \n",
    "                        if emission_prob != 0 and transition_prob != 0:\n",
    "                            new_prob = math.log(emission_prob) + math.log(transition_prob) + prev_prob\n",
    "                            max_k_paths.append((new_prob, prev_path + [k_prev]))\n",
    "            \n",
    "            max_k_paths.sort(key=lambda x: x[0], reverse=True)\n",
    "            viterbi_probs[i+1][j] = max_k_paths[:k]\n",
    "    \n",
    "    # Find top-k paths ending with \"STOP\"\n",
    "    max_k_paths = []\n",
    "    for tag in unique_tags:\n",
    "        if tag in viterbi_probs[len(test_token_seq_sentence)]:\n",
    "            for prob, path in viterbi_probs[len(test_token_seq_sentence)][tag]:\n",
    "                transition_prob = transition(transition_dictionary, tag, \"STOP\")\n",
    "                if transition_prob != 0:\n",
    "                    new_prob = prob + math.log(transition_prob)\n",
    "                    max_k_paths.append((new_prob, path + [tag]))\n",
    "    \n",
    "    max_k_paths.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_k_paths = max_k_paths[:k]\n",
    "    \n",
    "    return [path for _, path in top_k_paths]\n",
    "\n",
    "\n",
    "\n",
    "def viterbi_loop_k(test_token_seq, unique_tags, emission_dictionary, transition_dictionary, k):\n",
    "    result = []\n",
    "\n",
    "    for sentence in test_token_seq:\n",
    "        viterbi_paths = viterbi_k(sentence, unique_tags, emission_dictionary, transition_dictionary, k)\n",
    "        updated_paths = [handle_null_tags(sentence, path, unique_tags, emission_dictionary) for path in viterbi_paths]\n",
    "        result.append(updated_paths)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# def writeoutput_k_viterbi(filename, tokens, predicted_tags,k):\n",
    "#     with open(filename, 'w', encoding='utf-8') as file:\n",
    "\n",
    "#         for i in range(len(tokens)):\n",
    "#             print(i)\n",
    "#             for j in range(len(tokens[i])):\n",
    "#                 print(j)\n",
    "#                 print(tokens[i][j])\n",
    "#                 print(predicted_tags[i][1][j])\n",
    "#                 # file.write(tokens[i][j] + \" \" + predicted_tags[i][k][j] + \"\\n\" )\n",
    "\n",
    "#         # file.write(\"\\n\")\n",
    "\n",
    "def writeoutput_k_viterbi(filename, tokens, predicted_tags, k):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for i in range(len(tokens)):\n",
    "            if k > 0 and len(predicted_tags[i]) >= k:\n",
    "                tags_to_write = predicted_tags[i][k - 1]  # k-1 index for 0-based indexing\n",
    "            else:\n",
    "                # If there are fewer than k sequences available, write the last one available\n",
    "                tags_to_write = predicted_tags[i][-1] if predicted_tags[i] else []\n",
    "                \n",
    "            \n",
    "            for j in range(len(tokens[i])):\n",
    "                if j < len(tags_to_write):\n",
    "                    file.write(tokens[i][j] + \" \" + tags_to_write[j] + \"\\n\")\n",
    "                else:\n",
    "                    file.write(tokens[i][j] + \" \" + \"NULL\" + \"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 415\n",
      "\n",
      "#Correct Entity : 144\n",
      "Entity  precision: 0.3470\n",
      "Entity  recall: 0.3702\n",
      "Entity  F: 0.3582\n",
      "\n",
      "#Correct Sentiment : 100\n",
      "Sentiment  precision: 0.2410\n",
      "Sentiment  recall: 0.2571\n",
      "Sentiment  F: 0.2488\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")   \n",
    "ru_transition_dictionary = create_transition_dictionary(ru_tag_seq, ru_unique_tags)\n",
    "ru_predicted_tags_viterbi_k = viterbi_loop_k(ru_test_token_seq, ru_unique_tags, ru_emission_dictionary, ru_transition_dictionary , 2)\n",
    "\n",
    "writeoutput_k_viterbi(\"RU/dev.p3.2nd.out\", ru_test_token_seq, ru_predicted_tags_viterbi_k, 2)\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p3.2nd.out\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 546\n",
      "\n",
      "#Correct Entity : 128\n",
      "Entity  precision: 0.2344\n",
      "Entity  recall: 0.3290\n",
      "Entity  F: 0.2738\n",
      "\n",
      "#Correct Sentiment : 64\n",
      "Sentiment  precision: 0.1172\n",
      "Sentiment  recall: 0.1645\n",
      "Sentiment  F: 0.1369\n"
     ]
    }
   ],
   "source": [
    "ru_token_seq, ru_tag_seq = read_train(\"RU/train\")\n",
    "ru_emission_dictionary, ru_unique_tags = create_emission_dictionary(ru_token_seq, ru_tag_seq, 1)\n",
    "ru_test_token_seq = read_test(\"RU/dev.in\")   \n",
    "ru_transition_dictionary = create_transition_dictionary(ru_tag_seq, ru_unique_tags)\n",
    "ru_predicted_tags_viterbi_k = viterbi_loop_k(ru_test_token_seq, ru_unique_tags, ru_emission_dictionary, ru_transition_dictionary , 8)\n",
    "\n",
    "\n",
    "writeoutput_k_viterbi(\"RU/dev.p3.8th.out\", ru_test_token_seq, ru_predicted_tags_viterbi_k, 8)\n",
    "!python \"evalResult.py\" \"RU/dev.out\" \"RU/dev.p3.8th.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 \n",
    "Our approach is to train a new pytorch model \n",
    "We set 100 to the embedding dimension to have a better range of sensitivity to the relationships between each possible node\n",
    "and set 200 to the hidden dimension for a 200 layer RNN\n",
    "\n",
    "We set epoch to be 800 as it starts plateauing thereafter and batchsize to be 1000 to not be too big or too small for a stable gradient that is still sensitive to changes between dataset.\n",
    "\n",
    "It is trained using gpu on google collab link below\n",
    "\n",
    "https://colab.research.google.com/drive/1dsjMv1zOb1ruNEn0AfuFfSgZ1Kv1sYSo?usp=sharing\n",
    "\n",
    "We then loaded each test.in for RU and ES.\n",
    "\n",
    "To evaluate and generate the tags use the model russian_final.pt and espanol_final.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def create_bigram_transition_dictionary(tag_seq, unique_tags):\n",
    "    transition_dictionary = {}\n",
    "\n",
    "    for first_tag in unique_tags:\n",
    "        for second_tag in unique_tags:\n",
    "            count = 0\n",
    "            for i in tag_seq:\n",
    "                for j in range(len(i) - 1):\n",
    "                    if i[j] == first_tag and i[j+1] == second_tag:\n",
    "                        count += 1\n",
    "            if count != 0:\n",
    "                transition_dictionary[(first_tag, second_tag)] = count\n",
    "\n",
    "    start_dict = Counter(i[0] for i in tag_seq)\n",
    "    stop_dict = Counter(i[-1] for i in tag_seq)\n",
    "\n",
    "    total_sentences = len(tag_seq)\n",
    "\n",
    "    for tag, count in start_dict.items():\n",
    "        transition_dictionary[('START', tag)] = count\n",
    "\n",
    "    for tag, count in stop_dict.items():\n",
    "        transition_dictionary[(tag, 'STOP')] = count\n",
    "\n",
    "    return transition_dictionary\n",
    "\n",
    "def bigram_transition(transition_dict, previous_tag, current_tag):\n",
    "    transition_key = (previous_tag, current_tag)\n",
    "    \n",
    "    if transition_key not in transition_dict:\n",
    "        return math.log(1)  # Return a small value for unseen transitions\n",
    "    \n",
    "    total_count = sum(transition_dict[key] for key in transition_dict if key[0] == previous_tag)\n",
    "    return math.log(transition_dict[transition_key] / total_count)\n",
    "    \n",
    "def create_emission_dictionary(token_seq, tag_seq,k):\n",
    "    emission_dictionary = {} \n",
    "    count_tag = {} #number of words tagged with tag\n",
    "    count_token_tagged_tag = {} #number of times a token is tagged with tag\n",
    "    for i in range(0,len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not (y in count_tag.keys()):\n",
    "                count_tag[y] = 0\n",
    "\n",
    "            count_tag[y] += 1\n",
    "\n",
    "            if not (x in count_token_tagged_tag.keys()):\n",
    "                count_token_tagged_tag[x] = {}\n",
    "\n",
    "            if not (y in count_token_tagged_tag[x].keys()):\n",
    "                count_token_tagged_tag[x][y] = 0\n",
    "\n",
    "            count_token_tagged_tag[x][y] += 1\n",
    "    \n",
    "    for i in range(0, len(token_seq)):\n",
    "        for j in range(0, len(token_seq[i])):\n",
    "            x = token_seq[i][j]\n",
    "            y = tag_seq[i][j]\n",
    "            if not(x in emission_dictionary.keys()):\n",
    "                emission_dictionary[x] = {}\n",
    "            emission_dictionary[x][y] = (count_token_tagged_tag[x][y]) / (count_tag[y] + k)\n",
    "            emission_dictionary[x][\"START\"] = 0\n",
    "            emission_dictionary[x][\"STOP\"] = 0\n",
    "\n",
    "    emission_dictionary[\"#UNK#\"] = {}\n",
    "    for tag in count_tag.keys():\n",
    "        emission_dictionary[\"#UNK#\"][tag] = k / (count_tag[tag] + k)\n",
    "    uniquetags = list(count_tag.keys())\n",
    "    return emission_dictionary , uniquetags\n",
    "\n",
    "def emission(token, tag, emission_dictionary, uniquetags):\n",
    "    if tag not in uniquetags:\n",
    "        return math.log(1)\n",
    "    elif token not in emission_dictionary:\n",
    "        return emission_dictionary[\"#UNK#\"][tag]\n",
    "    elif tag not in emission_dictionary[token]:\n",
    "        return math.log(1)\n",
    "    else:\n",
    "        return emission_dictionary[token][tag]\n",
    "\n",
    "def viterbi(test_token_seq_sentence, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    viterbi_probs = {0: {\"START\": math.log(1)}, **{i: {tag: -math.inf for tag in unique_tags} for i in range(1, len(test_token_seq_sentence) + 1)}}\n",
    "    \n",
    "    # Calculate probabilities for each tag in the first word\n",
    "    for tag in unique_tags:\n",
    "        emission_prob = emission(test_token_seq_sentence[0], tag, emission_dictionary, unique_tags)\n",
    "        transition_prob = bigram_transition(transition_dictionary, \"START\", tag)  # Use bigram_transition\n",
    "        \n",
    "        if emission_prob == 0 or transition_prob == 0:\n",
    "            viterbi_probs[1][tag] = -math.inf\n",
    "        else:\n",
    "            viterbi_probs[1][tag] = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[0][\"START\"]\n",
    "            \n",
    "    # Calculate probabilities for each tag in the rest of the words\n",
    "    for i in range(1, len(test_token_seq_sentence)):\n",
    "        next_token = test_token_seq_sentence[i]\n",
    "        for j in unique_tags:\n",
    "            max_prob = -math.inf\n",
    "            for k in unique_tags:\n",
    "                emission_prob = emission(next_token, j, emission_dictionary, unique_tags)\n",
    "                transition_prob = bigram_transition(transition_dictionary, k, j)  # Use bigram_transition\n",
    "                \n",
    "                if emission_prob == 0 or transition_prob == 0:\n",
    "                    prob = -math.inf\n",
    "                else:\n",
    "                    prob = math.log(emission_prob) + math.log(transition_prob) + viterbi_probs[i][k]\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "            viterbi_probs[i+1][j] = max_prob\n",
    "    \n",
    "    # Calculate probabilities for transitioning to the \"STOP\" tag\n",
    "    max_prob = -math.inf\n",
    "    for tag in unique_tags:\n",
    "        previous_prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = bigram_transition(transition_dictionary, tag, \"STOP\")  # Use bigram_transition\n",
    "        \n",
    "        if transition_prob == 0:\n",
    "            prob = -math.inf\n",
    "        else:\n",
    "            prob = previous_prob + math.log(transition_prob)\n",
    "        \n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1] = {}\n",
    "    viterbi_probs[len(test_token_seq_sentence)+1][\"STOP\"] = max_prob\n",
    "    \n",
    "    # Backtracking to find the best path\n",
    "    best_path = []\n",
    "    argmax = -math.inf\n",
    "    currentmax = -math.inf\n",
    "    argmax_tag = \"NULL\"\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        prob = viterbi_probs[len(test_token_seq_sentence)][tag]\n",
    "        transition_prob = bigram_transition(transition_dictionary, tag, \"STOP\")  # Use bigram_transition\n",
    "        if transition_prob != 0:\n",
    "            currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "\n",
    "    best_path.append(argmax_tag)\n",
    "    \n",
    "    # Backpropagation\n",
    "    for i in range(len(test_token_seq_sentence), 1, -1):\n",
    "        argmax = -math.inf\n",
    "        currentmax = math.log(1)\n",
    "        \n",
    "        for tag in unique_tags:\n",
    "            prob = viterbi_probs[i-1][tag]\n",
    "            transition_prob = bigram_transition(transition_dictionary, tag, best_path[-1])  # Use bigram_transition\n",
    "            \n",
    "            if transition_prob == 0:\n",
    "                currentmax = -math.inf\n",
    "            else:\n",
    "                currentmax = prob + math.log(transition_prob)\n",
    "            \n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = tag\n",
    "        best_path.append(argmax_tag)\n",
    "\n",
    "    best_path.reverse()\n",
    "    return best_path\n",
    "\n",
    "def viterbi_loop(test_token_seq, unique_tags, emission_dictionary, transition_dictionary):\n",
    "    result = []\n",
    "\n",
    "    for sentence in test_token_seq:\n",
    "        viterbi_tags = viterbi(sentence, unique_tags, emission_dictionary, transition_dictionary)\n",
    "        updated_tags = handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary)\n",
    "        result.append(updated_tags)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m es_test_token_seq \u001b[39m=\u001b[39m read_test(\u001b[39m\"\u001b[39m\u001b[39mES/dev.in\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m es_bigram_transition_dictionary \u001b[39m=\u001b[39m create_bigram_transition_dictionary(es_tag_seq, es_unique_tags)  \u001b[39m# Create bigram transition dictionary\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m es_predicted_tags_bigram_viterbi \u001b[39m=\u001b[39m viterbi_loop(es_test_token_seq, es_unique_tags, es_emission_dictionary, es_bigram_transition_dictionary)  \u001b[39m# Use bigram emission and transition dictionaries\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Print the predicted tags\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sentence, tags \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(es_test_token_seq, es_predicted_tags_bigram_viterbi):\n",
      "Cell \u001b[1;32mIn[14], line 177\u001b[0m, in \u001b[0;36mviterbi_loop\u001b[1;34m(test_token_seq, unique_tags, emission_dictionary, transition_dictionary)\u001b[0m\n\u001b[0;32m    174\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m test_token_seq:\n\u001b[1;32m--> 177\u001b[0m     viterbi_tags \u001b[39m=\u001b[39m viterbi(sentence, unique_tags, emission_dictionary, transition_dictionary)\n\u001b[0;32m    178\u001b[0m     updated_tags \u001b[39m=\u001b[39m handle_null_tags(sentence, viterbi_tags, unique_tags, emission_dictionary)\n\u001b[0;32m    179\u001b[0m     result\u001b[39m.\u001b[39mappend(updated_tags)\n",
      "Cell \u001b[1;32mIn[14], line 97\u001b[0m, in \u001b[0;36mviterbi\u001b[1;34m(test_token_seq_sentence, unique_tags, emission_dictionary, transition_dictionary)\u001b[0m\n\u001b[0;32m     95\u001b[0m         viterbi_probs[\u001b[39m1\u001b[39m][tag] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf\n\u001b[0;32m     96\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m         viterbi_probs[\u001b[39m1\u001b[39m][tag] \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mlog(emission_prob) \u001b[39m+\u001b[39m math\u001b[39m.\u001b[39;49mlog(transition_prob) \u001b[39m+\u001b[39m viterbi_probs[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mSTART\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     99\u001b[0m \u001b[39m# Calculate probabilities for each tag in the rest of the words\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(test_token_seq_sentence)):\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "es_token_seq, es_tag_seq = read_train(\"ES/train\")\n",
    "es_emission_dictionary, es_unique_tags = create_emission_dictionary(es_token_seq, es_tag_seq, 1)  # This is unigram emission dictionary, you need to update it to bigram\n",
    "es_test_token_seq = read_test(\"ES/dev.in\")\n",
    "es_bigram_transition_dictionary = create_bigram_transition_dictionary(es_tag_seq, es_unique_tags)  # Create bigram transition dictionary\n",
    "es_predicted_tags_bigram_viterbi = viterbi_loop(es_test_token_seq, es_unique_tags, es_emission_dictionary, es_bigram_transition_dictionary)  # Use bigram emission and transition dictionaries\n",
    "\n",
    "# Print the predicted tags\n",
    "for sentence, tags in zip(es_test_token_seq, es_predicted_tags_bigram_viterbi):\n",
    "    print(\" \".join(sentence))\n",
    "    print(\" \".join(tags))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
